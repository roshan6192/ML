{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqFMcR3Zqla0",
        "outputId": "9db76bcd-5000-4710-f0f2-02ade52eb923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraper:\n",
        "  def __init__(self, url):\n",
        "    self.url = url\n",
        "\n",
        "  def extract_article_text(self):\n",
        "    response = requests.get(self.url)\n",
        "    html_content = response.content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    article_text = soup.get_text()\n",
        "    return article_text"
      ],
      "metadata": {
        "id": "GogANT_pq5ZY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "  def __init__(self, nltk_stopwords):\n",
        "    self.nltk_stopwords = nltk_stopwords\n",
        "\n",
        "  def tokenize_and_clean(self, text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
        "    return filtered_words\n",
        "\n"
      ],
      "metadata": {
        "id": "XHiOvhOGr0Ua"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ETLPipeline:\n",
        "  def __init__(self, url):\n",
        "    self.url = url\n",
        "    self.nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "  def run(self):\n",
        "      scraper = WebScraper(self.url)\n",
        "      article_text = scraper.extract_article_text()\n",
        "\n",
        "      processor = TextProcessor(self.nltk_stopwords)  # Assuming TextProcessor class exists\n",
        "      filtered_words = processor.tokenize_and_clean(article_text)\n",
        "\n",
        "      word_freq = Counter(filtered_words)\n",
        "      df = pd.DataFrame(word_freq.items(), columns=['Words','Frequency'])\n",
        "      df = df.sort_values(by='Frequency', ascending=False)\n",
        "      return df"
      ],
      "metadata": {
        "id": "KDCBPg1rsk81"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  article_url = input('Enter your url :')\n",
        "  if article_url:\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()\n",
        "    print(result_df.head())\n",
        "  else:\n",
        "    print(\"No URL provided. Please try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aKzZEZ5uULM",
        "outputId": "d95a0ced-9b1e-41b4-c8fd-405d1a6133b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your url :https://amankharwal.medium.com/what-is-time-series-analysis-in-data-science-f89aaa1c0814\n",
            "       Words  Frequency\n",
            "3       data         24\n",
            "1     series         24\n",
            "0       time         21\n",
            "2   analysis         14\n",
            "12   science          8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHavFE6au6J_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}